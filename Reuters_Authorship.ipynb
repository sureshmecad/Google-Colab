{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "48px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 4,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "name": "Reuters Authorship.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8NCVqhxKzVTo"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sureshmecad/Google-Colab/blob/master/Reuters_Authorship.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqtE9qCqztTi"
      },
      "source": [
        "# Week 2: Python Business Analytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGotMBbXz1Pl"
      },
      "source": [
        "See the Repository for Future Work: https://github.com/firmai/python-business-analytics or\n",
        "\n",
        "Sign up to the mailing list: https://mailchi.mp/ec4942d52cc5/firmai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki1UWVLczVCC"
      },
      "source": [
        "# Text Mining NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeorQOsbzVCQ"
      },
      "source": [
        "# *Introduction*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeKqM74hzVCR"
      },
      "source": [
        "This notebook contains code examples to get you started with Natural Language Processing (NLP) / Text Mining for Research and Data Science purposes.  \n",
        "\n",
        "In the large scheme of things there are roughly 4 steps:  \n",
        "\n",
        "1. Identify a data source  \n",
        "2. Gather the data  \n",
        "3. Process the data  \n",
        "4. Analyze the data  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7XU4fuOzVCT"
      },
      "source": [
        "## Note: companion slides"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W8FwKUgzVCZ"
      },
      "source": [
        "# *Elements / topics that are discussed in this notebook: *\n",
        "\n",
        "\n",
        "<img style=\"float: left\" src=\"https://i.imgur.com/c3aCZLA.png\" width=\"50%\" /> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD7WDs30zVCa"
      },
      "source": [
        "# *Table of Contents*  <a id='toc'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqFVyXnDzVCc"
      },
      "source": [
        "* [Primer on NLP tools](#tool_primer)     \n",
        "* [Process + Clean text](#proc_clean)   \n",
        "    * [Normalization](#normalization)\n",
        "        * [Deal with unwanted characters](#unwanted_char)\n",
        "        * [Sentence segmentation](#sentence_seg)   \n",
        "        * [Word tokenization](#word_token)\n",
        "        * [Lemmatization & Stemming](#lem_and_stem) \n",
        "    * [Language modeling](#lang_model) \n",
        "        * [Part-of-Speech tagging](#pos_tagging) \n",
        "        * [Uni-Gram & N-Grams](#n_grams) \n",
        "        * [Stop words](#stop_words) \n",
        "* [Direct feature extraction](#feature_extract) \n",
        "    * [Feature search](#feature_search) \n",
        "        * [Entity recognition](#entity_recognition) \n",
        "        * [Pattern search](#pattern_search) \n",
        "    * [Text evaluation](#text_eval) \n",
        "        * [Language](#language) \n",
        "        * [Dictionary counting](#dict_counting) \n",
        "        * [Readability](#readability) \n",
        "* [Represent text numerically](#text_numerical) \n",
        "    * [Bag of Words](#bows) \n",
        "        * [TF-IDF](#tfidf) \n",
        "    * [Word Embeddings](#word_embed) \n",
        "        * [Word2Vec](#Word2Vec) \n",
        "* [Statistical models](#stat_models) \n",
        "    * [\"Traditional\" machine learning](#trad_ml) \n",
        "        * [Supervised](#trad_ml_supervised) \n",
        "            * [Na√Øve Bayes](#trad_ml_supervised_nb) \n",
        "            * [Support Vector Machines (SVM)](#trad_ml_supervised_svm) \n",
        "        * [Unsupervised](#trad_ml_unsupervised) \n",
        "            * [Latent Dirichilet Allocation (LDA)](#trad_ml_unsupervised_lda) \n",
        "            * [pyLDAvis](#trad_ml_unsupervised_pyLDAvis) \n",
        "* [Model Selection and Evaluation](#trad_ml_eval) \n",
        "* [Neural Networks](#nn_ml)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "zJnQENtazVCf"
      },
      "source": [
        "# <span style=\"text-decoration: underline;\">Primer on NLP tools</span><a id='tool_primer'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbPFc05MzVCh"
      },
      "source": [
        "There are many tools available for NLP purposes.  \n",
        "The code examples below are based on what I personally like to use, it is not intended to be a comprehsnive overview.  \n",
        "\n",
        "Besides build-in Python functionality I will use / demonstrate the following packages:\n",
        "\n",
        "**Standard NLP libraries**:\n",
        "1. `Spacy` and the higher-level wrapper `Textacy` \n",
        "2. `NLTK` and the higher-level wrapper `TextBlob`\n",
        "\n",
        "*Note: besides installing the above packages you also often have to download (model) data . Make sure to check the documentation!*\n",
        "\n",
        "**Standard machine learning library**:\n",
        "\n",
        "1. `scikit learn`\n",
        "\n",
        "**Specific task libraries**:\n",
        "\n",
        "There are many, just a couple of examples:\n",
        "\n",
        "1. `pyLDAvis` for visualizing LDA)\n",
        "2. `langdetect` for detecting languages\n",
        "3. `fuzzywuzzy` for fuzzy text matching\n",
        "4. `textstat` to calculate readability statistics\n",
        "5. `Gensim` for topic modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Jj3UkiLSzVCj"
      },
      "source": [
        "# <span style=\"text-decoration: underline;\">Get some example data</span><a id='example_data'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po-N0OZKzVCm"
      },
      "source": [
        "There are many example datasets available to play around with, see for example this great repository:  \n",
        "https://archive.ics.uci.edu/ml/datasets.html?format=&task=&att=&area=&numAtt=&numIns=&type=text&sort=nameUp&view=table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9axpUNqqzVCp"
      },
      "source": [
        "The data that I will use for most of the examples is the \"Reuter_50_50 Data Set\" that is used for author identification experiments. \n",
        "\n",
        "See the details here: https://archive.ics.uci.edu/ml/datasets/Reuter_50_50  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_EQ6LkxzVCr"
      },
      "source": [
        "### Download and load the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fPhFIZozVCt"
      },
      "source": [
        "Can't follow what I am doing here? Please see my [Python tutorial](https://github.com/TiesdeKok/LearnPythonforResearch) (although the `zipfile` and `io` operations are not very relevant)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0ptU3H_7hdD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d7a449c4-6c2b-48a0-edce-4fb8f20f81ba"
      },
      "source": [
        "!pip install textacy\n",
        "!pip install langdetect\n",
        "!pip install gensim\n",
        "!pip install fuzzywuzzy\n",
        "!pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textacy in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.3)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (1.16.4)\n",
            "Requirement already satisfied: python-levenshtein>=0.12.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.12.0)\n",
            "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (3.1.1)\n",
            "Requirement already satisfied: pyphen>=0.9.4 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.9.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (1.3.0)\n",
            "Requirement already satisfied: spacy>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.1.6)\n",
            "Requirement already satisfied: ftfy<5.0.0,>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (4.4.3)\n",
            "Requirement already satisfied: cytoolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.10.0)\n",
            "Requirement already satisfied: pyemd>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.5.1)\n",
            "Requirement already satisfied: scikit-learn>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.20.3)\n",
            "Requirement already satisfied: ijson>=2.3 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.4)\n",
            "Requirement already satisfied: unidecode>=0.04.19 in /usr/local/lib/python3.6/dist-packages (from textacy) (1.1.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.21.0)\n",
            "Requirement already satisfied: tqdm>=4.11.1 in /usr/local/lib/python3.6/dist-packages (from textacy) (4.28.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.11->textacy) (4.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-levenshtein>=0.12.0->textacy) (41.0.1)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0->textacy) (7.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0->textacy) (0.2.2)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0->textacy) (0.9.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0->textacy) (2.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0->textacy) (1.0.2)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0->textacy) (2.0.1)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0->textacy) (0.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0->textacy) (0.0.7)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy<5.0.0,>=4.2.0->textacy) (0.1.7)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.6/dist-packages (from ftfy<5.0.0,>=4.2.0->textacy) (1.0.1)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz>=0.8.0->textacy) (0.10.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (1.24.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from html5lib->ftfy<5.0.0,>=4.2.0->textacy) (1.12.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from html5lib->ftfy<5.0.0,>=4.2.0->textacy) (0.5.1)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.6/dist-packages (1.0.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect) (1.12.0)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.3.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.8.4)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.16.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.9.189)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.21.0)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.189 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.189)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.8)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.189->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.189->boto3->smart-open>=1.2.1->gensim) (0.14)\n",
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.6/dist-packages (0.17.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-376c90ffdb92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install langdetect'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install gensim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install fuzzywuzzy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install pyLDAvis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_display_stdin_widget\u001b[0;34m(delay_millis)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m   \u001b[0mhide_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cell_remove_stdin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m   \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhide_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;31m# unique.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEwYopmNzVCv"
      },
      "source": [
        "import requests, zipfile, io, os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-8KFZ4qzVC-"
      },
      "source": [
        "*Download and extract the zip file with the data *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZjfB-bUzVDB"
      },
      "source": [
        "if not os.path.exists('C50test'):\n",
        "    r = requests.get(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00217/C50.zip\")\n",
        "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    z.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVRLr8EbzVDR"
      },
      "source": [
        "*Load the data into memory*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzwaWd8HzVDT"
      },
      "source": [
        "folder_dict = {'test' : 'C50test'}\n",
        "text_dict = {'test' : {}}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USqLZVb4zVDg"
      },
      "source": [
        "for label, folder in folder_dict.items():\n",
        "    authors = os.listdir(folder)\n",
        "    for author in authors:\n",
        "        text_files = os.listdir(os.path.join(folder, author))\n",
        "        for file in text_files:\n",
        "            with open(os.path.join(folder, author, file), 'r') as text_file:\n",
        "                text_dict[label].setdefault(author, []).append(' '.join(text_file.readlines()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pbB8sAqzVEA"
      },
      "source": [
        "*Note: the text comes pre-split per sentence, for the sake of example I undo this through `' '.join(text_file.readlines()`*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jZczhS4zVEI"
      },
      "source": [
        "text_dict['test']['TimFarrand'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "hcDfreU4zVE4"
      },
      "source": [
        "# <span style=\"text-decoration: underline;\">Process + Clean text</span><a id='proc_clean'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-wjbBW-zVE5"
      },
      "source": [
        "## Convert the text into a NLP representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDY9_DR8zVFA"
      },
      "source": [
        "We can use the text directly, but if want to use packages like `spacy` and `textblob` we first have to convert the text into a corresponding object.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP_gh4-XzVFC"
      },
      "source": [
        "### Spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6lbCCb4zVFE"
      },
      "source": [
        "**Note:** depending on the way that you installed the language models you will need to import it differently:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z7q_0kpzVFG"
      },
      "source": [
        "```\n",
        "from spacy.en import English\n",
        "parser = English()\n",
        "```\n",
        "OR\n",
        "```\n",
        "import en_core_web_sm\n",
        "parser = en_core_web_sm.load()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pj4GrH-1zVFI"
      },
      "source": [
        "import en_core_web_sm\n",
        "parser = en_core_web_sm.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oE4wAqeszVFW"
      },
      "source": [
        "Convert all text in the \"test\" sample to a `spacy` `doc` object using `parser()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2XCZmFbzVFY"
      },
      "source": [
        "spacy_text = {}\n",
        "for author, text_list in text_dict['test'].items():\n",
        "    spacy_text[author] = [parser(text) for text in text_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUP_HWzxzVFk"
      },
      "source": [
        "type(spacy_text['TimFarrand'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gR79Z6jzVFy"
      },
      "source": [
        "### NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Im0zp4IKzVF0"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_BSfJKezVF7"
      },
      "source": [
        "We can apply basic `nltk` operations directly to the text so we don't need to convert first. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzbRGbaazVGD"
      },
      "source": [
        "### TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVYhYEDUzVGI"
      },
      "source": [
        "from textblob import TextBlob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3h9cyqnjzVGT"
      },
      "source": [
        "Convert all text in the \"test\" sample to a `TextBlob` object using `TextBlob()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miCnaBPCzVGY"
      },
      "source": [
        "textblob_text = {}\n",
        "for author, text_list in text_dict['test'].items():\n",
        "    textblob_text[author] = [TextBlob(text) for text in text_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_STJ4QWizVGr"
      },
      "source": [
        "type(textblob_text['TimFarrand'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "s3eNuaFczVG0"
      },
      "source": [
        "## <span style=\"text-decoration: underline;\">Normalization</span><a id='normalization'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHlPtBa6zVG4"
      },
      "source": [
        "**Text normalization** describes the task of transforming the text into a different (more comparable) form.  \n",
        "\n",
        "This can imply many things, I will show a couple of things below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Gui1Vbh-zVG9"
      },
      "source": [
        "### <span style=\"text-decoration: underline;\">Deal with unwanted characters</span><a id='unwanted_char'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJqf_-unzVG_"
      },
      "source": [
        "You will often notice that there are characters that you don't want in your text.  \n",
        "\n",
        "Let's look at this sentence for example:\n",
        "\n",
        "> \"Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain\\'s Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts.\\n Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers\"\n",
        "\n",
        "You notice that there are some `\\` and `\\n` in there. These are used to define how a string should be displayed, if we print this text we get:  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI528gkQzVHD"
      },
      "source": [
        "text_dict['test']['TimFarrand'][0][:298]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ot36oM-YzVHM"
      },
      "source": [
        "print(text_dict['test']['TimFarrand'][0][:298])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48pFgqarzVHU"
      },
      "source": [
        "If we want to analyze text we often don't care about the visual representation. They might actually cause problems!  \n",
        "\n",
        "** So how do we remove them? **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foq49UGGzVHX"
      },
      "source": [
        "In many cases it is sufficient to simply use the `.replace()` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjf30lSZzVHZ"
      },
      "source": [
        "text_dict['test']['TimFarrand'][0][:298].replace('\\n', '').replace('\\\\', '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tyl9zCsAzVHn"
      },
      "source": [
        "Sometimes, however, the problem arrises because of encoding / decoding problems.  \n",
        "\n",
        "In those cases you can usually do something like:  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RGwL1nKzVHp"
      },
      "source": [
        "problem_sentence = 'This is some \\\\u03c0 text that has to be cleaned\\\\u2026! it\\\\u0027s annoying!'\n",
        "print(problem_sentence.encode().decode('unicode_escape').encode('ascii','ignore'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "hO0E7tL9zVH0"
      },
      "source": [
        "### <span style=\"text-decoration: underline;\">Sentence segmentation</span><a id='sentence_seg'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B7YG1mczVH-"
      },
      "source": [
        "Sentence segmentation means the task of splitting up the piece of text by sentence.  \n",
        "\n",
        "You could do this by splitting on the `.` symbol, but dots are used in many other cases as well so it is not very robust:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ufvwboczVIA"
      },
      "source": [
        "text_dict['test']['TimFarrand'][0][:550].split('.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtXXcCidzVIq"
      },
      "source": [
        "It is better to use a more sophisticated implementation such as the one by `Spacy`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xhqki3D5zVIr"
      },
      "source": [
        "example_paragraph = spacy_text['TimFarrand'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpMnK7ZdzVIx"
      },
      "source": [
        "sentence_list = [s for s in example_paragraph.sents]\n",
        "sentence_list[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMEdmpOozVI_"
      },
      "source": [
        "Notice that the returned object is still a `spacy` object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-t-CTMLzVJD"
      },
      "source": [
        "type(sentence_list[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hubnl9KzVJN"
      },
      "source": [
        "Apply to all texts (for use later on):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4ldqRWPzVJP"
      },
      "source": [
        "spacy_sentences = {}\n",
        "for author, text_list in spacy_text.items():\n",
        "    spacy_sentences[author] = [list(text.sents) for text in text_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK4F1nTCzVJd"
      },
      "source": [
        "spacy_sentences['TimFarrand'][0][:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "pnFio7DDzVJm"
      },
      "source": [
        "### <span style=\"text-decoration: underline;\">Word tokenization</span><a id='word_token'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHvcdrOfzVJp"
      },
      "source": [
        "Word tokenization means to split the sentence (or text) up into words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7yhJWfJzVJr"
      },
      "source": [
        "example_sentence = spacy_sentences['TimFarrand'][0][0]\n",
        "example_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ubhNWlnzVKj"
      },
      "source": [
        "A word is called a `token` in this context (hence `tokenization`), using `spacy`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NyYQtZLzVKk"
      },
      "source": [
        "token_list = [token for token in example_sentence]\n",
        "token_list[0:15]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "289WZS32zVKv"
      },
      "source": [
        "### <span style=\"text-decoration: underline;\">Lemmatization & Stemming</span><a id='lem_and_stem'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4Sqgia2zVK0"
      },
      "source": [
        "In some cases you want to convert a word (i.e. token) into a more general representation.  \n",
        "\n",
        "For example: convert \"car\", \"cars\", \"car's\", \"cars'\" all into the word `car`.\n",
        "\n",
        "This is generally done through lemmatization / stemming (different approaches trying to achieve a similar goal).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0R7wbGxzVK5"
      },
      "source": [
        "**Spacy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWo_LBdNzVLB"
      },
      "source": [
        "Space offers build-in functionality for lemmatization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjeVMKvDzVLC"
      },
      "source": [
        "lemmatized = [token.lemma_ for token in example_sentence]\n",
        "lemmatized[0:15]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUnihWkKzVLa"
      },
      "source": [
        "**NLTK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWRNB4e7zVLd"
      },
      "source": [
        "Using the NLTK libary we can also use the more aggressive Porter Stemmer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEqxYGvszVLe"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yMJq8eKzVLj"
      },
      "source": [
        "stemmed = [stemmer.stem(token.text) for token in example_sentence]\n",
        "stemmed[0:15]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOw9KWKbzVLz"
      },
      "source": [
        "**Compare**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWzVOxHMzVL0"
      },
      "source": [
        "for original, lemma, stem in zip(token_list[:15], lemmatized[:15], stemmed[:15]):\n",
        "    print(original, ' | ', lemma, ' | ', stem)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WST1UsmzVMA"
      },
      "source": [
        "In my experience it is usually best to use lemmatization instead of a stemmer. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "k1n7zXYRzVMC"
      },
      "source": [
        "## <span style=\"text-decoration: underline;\">Language modeling</span><a id='lang_model'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOTi2TTEzVMD"
      },
      "source": [
        "Text is inherently structured in complex ways, we can often use some of this underlying structure. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "OEiCmMoVzVMH"
      },
      "source": [
        "### <span style=\"text-decoration: underline;\">Part-of-Speech tagging</span><a id='pos_tagging'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJvGsa0DzVMI"
      },
      "source": [
        "Part of speech tagging refers to the identification of words as nouns, verbs, adjectives, etc. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONuqyRPvzVMQ"
      },
      "source": [
        "Using `Spacy`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4B7AUIxzVMR"
      },
      "source": [
        "pos_list = [(token, token.pos_) for token in example_sentence]\n",
        "pos_list[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "D152zwLUzVMX"
      },
      "source": [
        "### <span style=\"text-decoration: underline;\">Uni-Gram & N-Grams</span><a id='n_grams'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b-gWZCBzVMY"
      },
      "source": [
        "Obviously a sentence is not a random collection of words, the sequence of words has information value.  \n",
        "\n",
        "A simple way to incorporate some of this sequence is by using what is called `n-grams`.  \n",
        "An `n-gram` is nothing more than a a combination of `N` words into one token (a uni-gram token is just one word).  \n",
        "\n",
        "So we can convert `\"Sentence about flying cars\"` into a list of bigrams:\n",
        "\n",
        "> Sentence-about, about-flying, flying-cars  \n",
        "\n",
        "See my slide on N-Grams for a more comprehensive example: [click here](http://www.tiesdekok.com/AccountingNLP_Slides/#14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Pc-zTsdzVMZ"
      },
      "source": [
        "Using `NLTK`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNY2U2VyzVMa"
      },
      "source": [
        "bigram_list = ['-'.join(x) for x in nltk.bigrams([token.text for token in example_sentence])]\n",
        "bigram_list[10:15]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "jgy4iAQpzVMj"
      },
      "source": [
        "### <span style=\"text-decoration: underline;\">Stop words</span><a id='stop_words'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bif0cWA9zVMl"
      },
      "source": [
        "Depending on what you are trying to do it is possible that there are many words that don't add any information value to the sentence.  \n",
        "\n",
        "The primary example are stop words.  \n",
        "\n",
        "Sometimes you can improve the accuracy of your model by removing stop words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9m4Rq7QzVMo"
      },
      "source": [
        "Using `Spacy`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7w9yH9hzVMp"
      },
      "source": [
        "no_stop_words = [token for token in example_sentence if not token.is_stop]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1sYIDe-zVMu"
      },
      "source": [
        "no_stop_words[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpmZjNLXzVM-"
      },
      "source": [
        "token_list[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfyZoy-6zVNM"
      },
      "source": [
        "*Note* we can also remove punctuation in the same way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kd_rGgTzVNN"
      },
      "source": [
        "[token for token in example_sentence if not token.is_punct][:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnGJNZJWzVNV"
      },
      "source": [
        "## Wrap everything into one function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TN5Ti3nIzVNW"
      },
      "source": [
        "Below I will primarily use `SpaCy` directly. However, I also recommend to check out the high-level wrapper `Textacy`.\n",
        "\n",
        "See their GitHub page for details: https://github.com/chartbeat-labs/textacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZKHceWXzVNX"
      },
      "source": [
        "### Quick `Textacy` example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCVsa9_Q6CmU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikof63OBzVNY"
      },
      "source": [
        "import textacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwjvM34vzVNe"
      },
      "source": [
        "example_text = text_dict['test']['TimFarrand'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDUV2X1LzVNj"
      },
      "source": [
        "cleaned_text = textacy.preprocess_text(example_text, lowercase=True, fix_unicode=True, no_punct=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M16-lmobzVNo"
      },
      "source": [
        "** Basic SpaCy text processing function **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R8Q6c5rzVNq"
      },
      "source": [
        "1. Split into sentences\n",
        "2. Apply lemmatizer and remove top words\n",
        "3. Clean up the sentence using `textacy`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk2lMON0zVNs"
      },
      "source": [
        "def process_text_custom(text):\n",
        "    sentences = list(parser(text).sents)\n",
        "    lemmatized_sentences = []\n",
        "    for sentence in sentences:\n",
        "        lemmatized_sentences.append([token.lemma_ for token in sentence if not token.is_stop | token.is_punct | token.is_space])\n",
        "    return [parser(' '.join(sentence)) for sentence in lemmatized_sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HwX9R_YzVNw"
      },
      "source": [
        "%%time\n",
        "spacy_text_clean = {}\n",
        "for author, text_list in text_dict['test'].items():\n",
        "    lst = []\n",
        "    for text in text_list:\n",
        "        lst.append(process_text_custom(text))\n",
        "    spacy_text_clean[author] = lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OK3ybv1zVN5"
      },
      "source": [
        "Note that there are quite a lot of sentences (~52K) so this takes a bit of time (~ 15 minutes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yLy5CSlzVN6"
      },
      "source": [
        "count = 0\n",
        "for author, texts in spacy_text_clean.items():\n",
        "    for text in texts:\n",
        "        count += len(text)\n",
        "print('Number of sentences:', count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov_YJypWzVOO"
      },
      "source": [
        "Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABxccKHezVOP"
      },
      "source": [
        "spacy_text_clean['TimFarrand'][0][:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Uo89NizNzVOY"
      },
      "source": [
        "# <span style=\"text-decoration: underline;\">Direct feature extraction</span><a id='feature_extract'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJMPsyLwzVOZ"
      },
      "source": [
        "We now have pre-processed our text into something that we can use for direct feature extraction or to convert it to a numerical representation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "vefZY_WbzVOa"
      },
      "source": [
        "## <span style=\"text-decoration: underline;\">Feature search</span><a id='feature_search'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "zvB_TQqwzVOb"
      },
      "source": [
        "### <span style=\"text-decoration: underline;\">Entity recognition</span><a id='entity_recognition'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkHbQjBWzVOc"
      },
      "source": [
        "It is often useful / relevant to extract entities that are mentioned in a piece of text.   \n",
        "\n",
        "SpaCy is quite powerful in extracting entities, however, it doesn't work very well on lowercase text.  \n",
        "\n",
        "Given that \"token.lemma\\_\" removes capitalization I will use `spacy_sentences` for this example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-57j90XDzVOd"
      },
      "source": [
        "example_sentence = spacy_sentences['TimFarrand'][0][3]\n",
        "example_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUFq1a10zVOp"
      },
      "source": [
        "[(i, i.label_) for i in parser(example_sentence.text).ents]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFlwjICAzVOv"
      },
      "source": [
        "example_sentence = spacy_sentences['TimFarrand'][4][0]\n",
        "example_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTM4MjgTzVO5"
      },
      "source": [
        "[(i, i.label_) for i in parser(example_sentence.text).ents]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Phyb4rbpzVPM"
      },
      "source": [
        "### <span style=\"text-decoration: underline;\">Pattern search</span><a id='pattern_search'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Odw0uoyzVPN"
      },
      "source": [
        "Using the build-in `re` (regular expression) library you can pattern match nearly anything you want.  \n",
        "\n",
        "I will not go into details about regular expressions but see here for a tutorial:  \n",
        "https://regexone.com/references/python  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZs3HpaAzVPO"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0PW3WB9zVPU"
      },
      "source": [
        "**TIP**: Use [Pythex.org](https://pythex.org/) to try out your regular expression\n",
        "\n",
        "Example on Pythex: <a href=\"https://pythex.org/?regex=IDNUMBER: (\\d\\d\\d-\\w\\w)&test_string=Ties de Kok (IDNUMBER: 123-AZ). Rest of Text.\" target='_blank'>click here</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFiKKAZrzVPX"
      },
      "source": [
        "**Example 1:**  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5_ek_NkzVPY"
      },
      "source": [
        "string_1 = 'Ties de Kok (#IDNUMBER: 123-AZ). Rest of text...'\n",
        "string_2 = 'Philip Joos (#IDNUMBER: 663-BY). Rest of text...'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywReAZ9IzVPd"
      },
      "source": [
        "pattern = r'#IDNUMBER: (\\d\\d\\d-\\w\\w)'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXyhq2CIzVPh"
      },
      "source": [
        "print(re.findall(pattern, string_1)[0])\n",
        "print(re.findall(pattern, string_2)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_vzcmowzVQL"
      },
      "source": [
        "### Example 2:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOjI6qqkzVQM"
      },
      "source": [
        "If a sentence contains the word 'million' return True, otherwise return False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "RhtvMohizVQP"
      },
      "source": [
        "for sen in spacy_text_clean['TimFarrand'][2]:\n",
        "    TERM = 'million'\n",
        "    contains = True if re.search('million', sen.text) else False\n",
        "    if contains:\n",
        "        print(sen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "_kAH64jTzVQc"
      },
      "source": [
        "## <span style=\"text-decoration: underline;\">Text evaluation</span><a id='text_eval'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGXV6jb_zVQd"
      },
      "source": [
        "Besides feature search there are also many ways to analyze the text as a whole.  \n",
        "\n",
        "Let's, for example, evaluate the following paragraph:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ1waxmozVQf"
      },
      "source": [
        "example_paragraph = ' '.join([x.text for x in spacy_text_clean['TimFarrand'][2]])\n",
        "example_paragraph[:500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "7CbA_Uj4zVQv"
      },
      "source": [
        "### <span style=\"text-decoration: underline;\">Language</span><a id='language'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-h5gNZHzVQw"
      },
      "source": [
        "Using the `langdetect` package it is easy to detect the language of a piece of text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NbTq0U3_IVA"
      },
      "source": [
        "!pip install langdetect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAEVJFYyzVQx"
      },
      "source": [
        "from langdetect import detect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlqALfz7zVQ0"
      },
      "source": [
        "detect(example_paragraph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "frmtGDTPzVRE"
      },
      "source": [
        "### <span style=\"text-decoration: underline;\">Readability</span><a id='readability'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDKalz3yzVRN"
      },
      "source": [
        "Using the `textstat` package we can compute various readability metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWC-8mwTzVRO"
      },
      "source": [
        "https://github.com/shivam5992/textstat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPo_GBg7C5CM"
      },
      "source": [
        "!pip install textstat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVTjORoHzVRO"
      },
      "source": [
        "from textstat.textstat import textstat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "gc9LPUVHzVRS"
      },
      "source": [
        "print(textstat.flesch_reading_ease(example_paragraph))\n",
        "print(textstat.smog_index(example_paragraph))\n",
        "print(textstat.flesch_kincaid_grade(example_paragraph))\n",
        "print(textstat.coleman_liau_index(example_paragraph))\n",
        "print(textstat.automated_readability_index(example_paragraph))\n",
        "print(textstat.dale_chall_readability_score(example_paragraph))\n",
        "print(textstat.difficult_words(example_paragraph))\n",
        "print(textstat.linsear_write_formula(example_paragraph))\n",
        "print(textstat.gunning_fog(example_paragraph))\n",
        "print(textstat.text_standard(example_paragraph))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPO_VVHrzVRZ"
      },
      "source": [
        "## Text similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07d0tYdGDCn-"
      },
      "source": [
        "!pip install gensim\n",
        "!pip install fuzzywuzzy\n",
        "!pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m81Jm4m4zVRf"
      },
      "source": [
        "from fuzzywuzzy import fuzz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2vblIgozVRi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "db8a2323-fc05-4840-88d0-ad0d2580e133"
      },
      "source": [
        "fuzz.ratio(\"fuzzy wuzzy was a bear\", \"wuzzy fuzzy was a bear\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "91"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "a6c5W0NVzVRn"
      },
      "source": [
        "### <span style=\"text-decoration: underline;\">Term (dictionary) counting</span><a id='dict_counting'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTNiB1uqzVRo"
      },
      "source": [
        "One of the most common techniques that researchers currently use (at least in Accounting research) are simple metrics based on counting words in a dictionary.  \n",
        "This technique is, for example, very prevalent in sentiment analysis (counting positive and negative words).  \n",
        "\n",
        "In essence this technique is very simple to program:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ_Gm7v-zVRp"
      },
      "source": [
        "### Example 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WboJdEXxzVRs"
      },
      "source": [
        "word_dictionary = ['soft', 'first', 'most', 'be']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZFDFOIezVRv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "c3230d09-7438-475b-a3d5-9879880fae75"
      },
      "source": [
        "for word in word_dictionary:\n",
        "    print(word, example_paragraph.count(word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "soft 0\n",
            "first 0\n",
            "most 0\n",
            "be 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGv6r4tAzVR9"
      },
      "source": [
        "### Example 2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45YS4EmDzVR-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "abdb881c-f759-4d0f-f626-a702615d424a"
      },
      "source": [
        "pos = ['great', 'increase']\n",
        "neg = ['bad', 'decrease']\n",
        "\n",
        "sentence = '''According to Trump everything is great, great, \n",
        "and great even though his popularity is seeing a decrease.'''\n",
        "\n",
        "pos_count = 0\n",
        "for word in pos:\n",
        "    pos_count += sentence.lower().count(word)\n",
        "print(pos_count)\n",
        "\n",
        "neg_count = 0\n",
        "for word in neg:\n",
        "    neg_count += sentence.lower().count(word)\n",
        "print(neg_count)\n",
        "\n",
        "pos_count / (neg_count + pos_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.75"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TybOMjAQzVSH"
      },
      "source": [
        "sentence = '''According to Trump everything is great, great, \n",
        "and great even though his popularity is seeing a decrease.'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCBJuvipzVSK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9121dd43-79f5-448f-98fe-dae23503b390"
      },
      "source": [
        "pos_count = 0\n",
        "for word in pos:\n",
        "    pos_count += sentence.lower().count(word)\n",
        "print(pos_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by03-OIVzVSR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e223d80b-d636-4a9f-c537-d9ab5850cd28"
      },
      "source": [
        "neg_count = 0\n",
        "for word in neg:\n",
        "    neg_count += sentence.lower().count(word)\n",
        "print(neg_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMiSUvOYzVSX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "652a7451-3cce-4167-d3e2-5a3276a10f41"
      },
      "source": [
        "pos_count / (neg_count + pos_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.75"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCmhJIbszVSc"
      },
      "source": [
        "Getting the total number of words is also easy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cj1GiPsxzVSd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "492bbe68-1976-4c9b-f4bf-7c02901c68ec"
      },
      "source": [
        "len(parser(example_paragraph))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "370"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NCVqhxKzVTo"
      },
      "source": [
        "#### Example 3:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYMkGWDtzVTo"
      },
      "source": [
        "We can also save the count per word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV75kU7nzVTq"
      },
      "source": [
        "pos_count_dict = {}\n",
        "for word in pos:\n",
        "    pos_count_dict[word] = sentence.lower().count(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ctUf44xzVTt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eaca5e7-9ac4-4019-9d5f-6f870a6f7c65"
      },
      "source": [
        "pos_count_dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'great': 3, 'increase': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "HV_eHMHxzVUu"
      },
      "source": [
        "# <span style=\"text-decoration: underline;\">Represent text numerically</span><a id='text_numerical'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Or4hmJKyzVVC"
      },
      "source": [
        "## <span style=\"text-decoration: underline;\">Bag of Words</span><a id='bows'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_cQOeC2zVVH"
      },
      "source": [
        "Sklearn includes the `CountVectorizer` and `TfidfVectorizer` function.  \n",
        "\n",
        "For details, see the documentation:  \n",
        "[TF](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)  \n",
        "[TFIDF](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)\n",
        "\n",
        "Note 1: these functions also already includes a lot of preprocessing options (e.g. ngrames, remove stop words, accent stripper).\n",
        "\n",
        "Note 2: example based on the following website [click here](http://ethen8181.github.io/machine-learning/clustering_old/tf_idf/tf_idf.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo5GO5o-zVVY"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cR6YA_aUzVVe"
      },
      "source": [
        "### Simple example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpy0oD0NzVVf"
      },
      "source": [
        "doc_1 = \"The sky is blue.\"\n",
        "doc_2 = \"The sun is bright today.\"\n",
        "doc_3 = \"The sun in the sky is bright.\"\n",
        "doc_4 = \"We can see the shining sun, the bright sun.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVTX0FTezVVp"
      },
      "source": [
        "Calculate term frequency:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wMnrgmlzVVq"
      },
      "source": [
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "tf = vectorizer.fit_transform([doc_1, doc_2, doc_3, doc_4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Lm2LlYNzVVy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "a274f7b1-4d5f-4c39-dddb-8b6f0d5c0f30"
      },
      "source": [
        "print(vectorizer.get_feature_names())\n",
        "for doc_tf_vector in tf.toarray():\n",
        "    print(doc_tf_vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['blue', 'bright', 'shining', 'sky', 'sun', 'today']\n",
            "[1 0 0 1 0 0]\n",
            "[0 1 0 0 1 1]\n",
            "[0 1 0 1 1 0]\n",
            "[0 1 1 0 2 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "GPRbzfvJzVWT"
      },
      "source": [
        "### <span style=\"text-decoration: underline;\">TF-IDF</span><a id='tfidf'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQR-cppDzVWU"
      },
      "source": [
        "transformer = TfidfVectorizer(stop_words='english')\n",
        "tfidf = transformer.fit_transform([doc_1, doc_2, doc_3, doc_4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "id": "DP7xLrCKzVWY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "114f3664-b792-404d-c230-cc3cd4107ba8"
      },
      "source": [
        "for doc_vector in tfidf.toarray():\n",
        "    print(doc_vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.78528828 0.         0.         0.6191303  0.         0.        ]\n",
            "[0.         0.47380449 0.         0.         0.47380449 0.74230628]\n",
            "[0.         0.53256952 0.         0.65782931 0.53256952 0.        ]\n",
            "[0.         0.36626037 0.57381765 0.         0.73252075 0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRCeQpwVzVWd"
      },
      "source": [
        "### More elaborate example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLQYa1iFzVWe"
      },
      "source": [
        "clean_paragraphs = []\n",
        "for author, value in spacy_text_clean.items():\n",
        "    for article in value:\n",
        "        clean_paragraphs.append(' '.join([x.text for x in article]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XueJvbeZzVWh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a408eada-ea06-496a-c7c5-6cdbab7a401b"
      },
      "source": [
        "len(clean_paragraphs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99XHAHCFzVWl"
      },
      "source": [
        "transformer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_large = transformer.fit_transform(clean_paragraphs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5oOQ889zVWp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1d1acfd4-66f1-4f3a-f2ef-3392878b26f1"
      },
      "source": [
        "print('Number of vectors:', len(tfidf_large.toarray()))\n",
        "print('Number of words in dictionary:', len(tfidf_large.toarray()[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of vectors: 2500\n",
            "Number of words in dictionary: 24130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Akku50qvzVWt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b239fb66-e499-4465-d6a3-935a1cd056ac"
      },
      "source": [
        "tfidf_large"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<2500x24130 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 444178 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "89OXigz8zVWy"
      },
      "source": [
        "## <span style=\"text-decoration: underline;\">Word Embeddings</span><a id='word_embed'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "VxwA3sptzVWz"
      },
      "source": [
        "### <span style=\"text-decoration: underline;\">Word2Vec</span><a id='Word2Vec'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnpmYO8izVW0"
      },
      "source": [
        "Simple example below is from:  https://medium.com/@mishra.thedeepak/word2vec-in-minutes-gensim-nlp-python-6940f4e00980"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8RousiXzVW1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a3c6220a-e17d-43a4-d64f-2582eac73515"
      },
      "source": [
        "import gensim\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPBPtxWqzVW_"
      },
      "source": [
        "sentences = brown.sents()\n",
        "model = gensim.models.Word2Vec(sentences, min_count=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXtzmP__zVXE"
      },
      "source": [
        "Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP_TJbkwzVXI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "1f285964-91d3-4ce0-a503-c975d13cbf88"
      },
      "source": [
        "model.save('brown_model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI-mzHzkzVXT"
      },
      "source": [
        "Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-BUV7FPzVXW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "fb21fdd6-bd26-4318-fd44-18b63572c0d7"
      },
      "source": [
        "model = gensim.models.Word2Vec.load('brown_model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ81a8jPzVXd"
      },
      "source": [
        "Find words most similar to 'mother':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7Dk7RBgzVXd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "50dba550-4078-4c3a-9536-e72c3c977815"
      },
      "source": [
        "print(model.most_similar(\"mother\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('father', 0.9841663837432861), ('husband', 0.9670721292495728), ('wife', 0.9487313032150269), ('friend', 0.9323579668998718), ('son', 0.9275298714637756), ('nickname', 0.9200363159179688), ('eagle', 0.9182674288749695), ('addiction', 0.9054847955703735), ('voice', 0.9040984511375427), ('patient', 0.8997060060501099)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmI5IFgmzVXh"
      },
      "source": [
        "Find the odd one out:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-mbhBujzVXi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "30d71cdb-66a8-4829-a592-d47b9ccb4663"
      },
      "source": [
        "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cereal\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nq_qc0AzVXx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "9c69724d-8635-4b1d-8ba6-bac69e3cd5eb"
      },
      "source": [
        "print(model.doesnt_match(\"pizza pasta garden fries\".split()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "garden\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8NJT85TzVX4"
      },
      "source": [
        "Retrieve vector representation of the word \"human\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84jajA9wzVX5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "55fbab38-0c14-4362-f705-0253589551e2"
      },
      "source": [
        "model['human']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.2541619 ,  2.2159684 ,  1.047784  , -0.71238947, -0.61424005,\n",
              "       -0.2838337 ,  0.22731484,  0.64372134, -0.5807566 , -0.2922792 ,\n",
              "       -0.4993856 ,  0.19945972,  0.5109125 , -0.44036752, -0.31287178,\n",
              "       -0.05132972,  1.0744963 ,  0.49401098, -0.7575154 , -0.7520932 ,\n",
              "       -0.47252244,  0.15777719,  0.01892141, -0.13521086,  0.13225318,\n",
              "        0.01569277,  0.8201043 , -0.18367213, -0.5115785 ,  0.0950046 ,\n",
              "       -0.901529  , -0.00576999, -0.18296817, -0.7227348 , -0.36876544,\n",
              "        0.12904815, -0.49410135, -0.16567625,  0.25800195, -0.9476387 ,\n",
              "        0.6677448 , -0.66520613,  0.15521108, -0.05746429, -0.66669667,\n",
              "        1.1473489 , -0.30393326,  0.27609795,  0.03071395,  0.21279913,\n",
              "       -0.15011618,  0.06648927,  0.4653522 , -0.06295931, -0.59686804,\n",
              "        0.22332567,  0.52038115,  0.08707199, -0.03864726, -1.1777682 ,\n",
              "       -0.6586736 ,  0.7845623 ,  0.54146487, -1.0455779 , -0.5684107 ,\n",
              "        0.0466442 ,  0.44047025, -0.28766677, -0.17281069, -0.18058509,\n",
              "        0.21136013,  0.95359594, -0.66299623, -0.28144175,  0.01736428,\n",
              "        0.9233736 ,  0.533335  ,  0.47873688,  0.30168334, -1.0639472 ,\n",
              "        0.03699052,  0.32431152,  0.9761932 ,  0.6572108 , -0.32395357,\n",
              "       -0.21990493,  0.82805634,  0.27187476, -0.50445306,  0.06266934,\n",
              "       -0.05255144,  0.45138708,  0.34316576, -0.5600966 , -0.52912354,\n",
              "        0.18519725, -0.36004648, -0.4918508 , -0.04166657,  0.17255953],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "6Eht6I1JzVYG"
      },
      "source": [
        "# <span style=\"text-decoration: underline;\">Statistical models</span><a id='stat_models'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "iQTGfmcHzVYH"
      },
      "source": [
        "## <span style=\"text-decoration: underline;\">\"Traditional\" machine learning</span><a id='trad_ml'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paklyzdrzVYI"
      },
      "source": [
        "The library to use for machine learning is scikit-learn ([\"sklearn\"](http://scikit-learn.org/stable/index.html))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "mY1gvLnozVYI"
      },
      "source": [
        "## <span>Supervised</span><a id='trad_ml_supervised'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vyvdgww1zVYJ"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import metrics\n",
        "from sklearn.externals import joblib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47LgZmhMzVYS"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyNejarAzVYr"
      },
      "source": [
        "### Convert the data into a pandas dataframe (so that we can input it easier)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwRfyuMazVYv"
      },
      "source": [
        "article_list = []\n",
        "for author, value in spacy_text_clean.items():\n",
        "    for article in value:\n",
        "        article_list.append((author, ' '.join([x.text for x in article])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjMTWkorzVYz"
      },
      "source": [
        "article_df = pd.DataFrame(article_list, columns=['author', 'text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ukjf2aY0zVY2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "47e30415-9e0d-40d5-ee99-17a54813804c"
      },
      "source": [
        "article_df.sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>925</th>\n",
              "      <td>WilliamKazer</td>\n",
              "      <td>United States China verge breakthrough testy t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>584</th>\n",
              "      <td>SimonCowell</td>\n",
              "      <td>Lloyd London announce Tuesday backer time meet...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1000</th>\n",
              "      <td>DarrenSchuettler</td>\n",
              "      <td>Canada security regulator say Thursday probe B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>968</th>\n",
              "      <td>TheresePoletti</td>\n",
              "      <td>Microsoft Corp. big corporate foe include Inte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1600</th>\n",
              "      <td>JonathanBirt</td>\n",
              "      <td>drug discovery company Chiroscience Group Plc ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                author                                               text\n",
              "925       WilliamKazer  United States China verge breakthrough testy t...\n",
              "584        SimonCowell  Lloyd London announce Tuesday backer time meet...\n",
              "1000  DarrenSchuettler  Canada security regulator say Thursday probe B...\n",
              "968     TheresePoletti  Microsoft Corp. big corporate foe include Inte...\n",
              "1600      JonathanBirt  drug discovery company Chiroscience Group Plc ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aZZcgY0zVY_"
      },
      "source": [
        "### Split the sample into a training and test sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8Gr-Y1fzVZA"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(article_df.text, article_df.author, test_size=0.20, random_state=3561)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_m0XbFXzVZD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bd2c77ee-c944-4db4-9d51-9c7c2c803e23"
      },
      "source": [
        "print(len(X_train), len(X_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VuusccEzVZG"
      },
      "source": [
        "### Train and evaluate function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKkWHuvZzVZI"
      },
      "source": [
        "Simple function to train (i.e. fit) and evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASW0B6sjzVZI"
      },
      "source": [
        "def train_and_evaluate(clf, X_train, X_test, y_train, y_test):\n",
        "    \n",
        "    clf.fit(X_train, y_train)\n",
        "    \n",
        "    print(\"Accuracy on training set:\")\n",
        "    print(clf.score(X_train, y_train))\n",
        "    print(\"Accuracy on testing set:\")\n",
        "    print(clf.score(X_test, y_test))\n",
        "    \n",
        "    y_pred = clf.predict(X_test)\n",
        "    \n",
        "    print(\"Classification Report:\")\n",
        "    print(metrics.classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "uSwaI8sdzVZW"
      },
      "source": [
        "### <span>Na√Øve Bayes estimator</span><a id='trad_ml_supervised_nb'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3l7pDZpzVZX"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXsEg9LXzVZZ"
      },
      "source": [
        "Define pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O29a1MF5zVZZ"
      },
      "source": [
        "clf = Pipeline([\n",
        "    ('vect', TfidfVectorizer(strip_accents='unicode',\n",
        "                             lowercase = True,\n",
        "                            max_features = 1500,\n",
        "                            stop_words='english'\n",
        "                            )),\n",
        "        \n",
        "    ('clf', MultinomialNB(alpha = 1,\n",
        "                          fit_prior = True\n",
        "                          )\n",
        "    ),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdfKwR-3zVZc"
      },
      "source": [
        "Train and show evaluation stats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5R-dmEqzVZq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e63b90dd-0c4d-43f6-d0a7-4dab398141b6"
      },
      "source": [
        "train_and_evaluate(clf, X_train, X_test, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on training set:\n",
            "0.8405\n",
            "Accuracy on testing set:\n",
            "0.696\n",
            "Classification Report:\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "    AaronPressman       0.88      0.70      0.78        10\n",
            "       AlanCrosby       0.55      0.67      0.60         9\n",
            "   AlexanderSmith       0.71      0.56      0.63         9\n",
            "  BenjaminKangLim       0.67      0.18      0.29        11\n",
            "    BernardHickey       0.75      0.67      0.71         9\n",
            "      BradDorfman       0.90      0.90      0.90        10\n",
            " DarrenSchuettler       0.73      0.62      0.67        13\n",
            "      DavidLawder       0.89      0.62      0.73        13\n",
            "    EdnaFernandes       1.00      0.41      0.58        17\n",
            "      EricAuchard       0.80      0.44      0.57         9\n",
            "   FumikoFujisaki       0.86      0.86      0.86         7\n",
            "   GrahamEarnshaw       0.60      1.00      0.75         9\n",
            " HeatherScoffield       1.00      0.58      0.74        12\n",
            "       JanLopatka       0.62      0.45      0.53        11\n",
            "    JaneMacartney       0.40      0.50      0.44         8\n",
            "     JimGilchrist       0.92      1.00      0.96        12\n",
            "   JoWinterbottom       0.64      0.90      0.75        10\n",
            "         JoeOrtiz       0.70      0.88      0.78         8\n",
            "     JohnMastrini       0.58      0.78      0.67         9\n",
            "     JonathanBirt       0.83      0.71      0.77        14\n",
            "      KarlPenhaul       1.00      1.00      1.00        10\n",
            "        KeithWeir       0.45      0.83      0.59         6\n",
            "   KevinDrawbaugh       0.20      0.40      0.27         5\n",
            "    KevinMorrison       0.71      0.91      0.80        11\n",
            "    KirstinRidley       0.64      0.64      0.64        14\n",
            "KouroshKarimkhany       0.90      0.90      0.90        10\n",
            "        LydiaZajc       0.58      0.78      0.67         9\n",
            "   LynneO'Donnell       0.86      0.86      0.86        14\n",
            "  LynnleyBrowning       0.91      1.00      0.95        10\n",
            "  MarcelMichelson       0.67      0.80      0.73        10\n",
            "     MarkBendeich       0.86      0.67      0.75         9\n",
            "       MartinWolk       0.67      0.20      0.31        10\n",
            "     MatthewBunce       1.00      1.00      1.00        10\n",
            "    MichaelConnor       0.67      0.60      0.63        10\n",
            "       MureDickie       0.67      0.46      0.55        13\n",
            "        NickLouth       0.64      0.90      0.75        10\n",
            "  PatriciaCommins       0.57      0.89      0.70         9\n",
            "    PeterHumphrey       0.44      0.70      0.54        10\n",
            "       PierreTran       1.00      0.62      0.76        13\n",
            "       RobinSidel       1.00      0.78      0.88         9\n",
            "     RogerFillion       1.00      0.88      0.93         8\n",
            "      SamuelPerry       0.54      0.64      0.58        11\n",
            "     SarahDavison       0.83      0.42      0.56        12\n",
            "      ScottHillis       0.22      0.67      0.33         3\n",
            "      SimonCowell       0.71      0.50      0.59        10\n",
            "         TanEeLyn       0.56      0.62      0.59         8\n",
            "   TheresePoletti       0.83      1.00      0.91        10\n",
            "       TimFarrand       0.60      1.00      0.75         9\n",
            "       ToddNissen       0.53      0.89      0.67         9\n",
            "     WilliamKazer       0.20      0.12      0.15         8\n",
            "\n",
            "        micro avg       0.70      0.70      0.70       500\n",
            "        macro avg       0.71      0.70      0.68       500\n",
            "     weighted avg       0.74      0.70      0.69       500\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ximtzrGXzVaJ"
      },
      "source": [
        "Save results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDZnSjVczVaJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9357f7ab-b449-4534-e0e4-d6eecd962104"
      },
      "source": [
        "joblib.dump(clf, 'naive_bayes_results.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['naive_bayes_results.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWUzMTi0zVaN"
      },
      "source": [
        "Predict out of sample:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDAtLlu8zVaO"
      },
      "source": [
        "example_y, example_X = y_train[33], X_train[33]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OE1Kcc5HzVaU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9bcfe54c-9e47-4f7f-b764-ff0b95b64a6a"
      },
      "source": [
        "print('Actual author:', example_y)\n",
        "print('Predicted author:', clf.predict([example_X])[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual author: EricAuchard\n",
            "Predicted author: EricAuchard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "zEih7o-RzVaa"
      },
      "source": [
        "### <span>Support Vector Machines (SVM)</span><a id='trad_ml_supervised_svm'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3tWA3FizVab"
      },
      "source": [
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk2xP8ZdzVal"
      },
      "source": [
        "Define pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtWDmCKFzVal"
      },
      "source": [
        "clf_svm = Pipeline([\n",
        "    ('vect', TfidfVectorizer(strip_accents='unicode',\n",
        "                             lowercase = True,\n",
        "                            max_features = 1500,\n",
        "                            stop_words='english'\n",
        "                            )),\n",
        "        \n",
        "    ('clf', SVC(kernel='rbf' ,\n",
        "                C=10, gamma=0.3)\n",
        "    ),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUnKn1VfzVao"
      },
      "source": [
        "*Note:* The SVC estimator is very sensitive to the hyperparameters!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCMbZQU7zVas"
      },
      "source": [
        "Train and show evaluation stats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYAILjS5zVat",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "980ffe75-c52e-4d2b-8b93-af020b1046ca"
      },
      "source": [
        "train_and_evaluate(clf_svm, X_train, X_test, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on training set:\n",
            "0.998\n",
            "Accuracy on testing set:\n",
            "0.828\n",
            "Classification Report:\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "    AaronPressman       0.82      0.90      0.86        10\n",
            "       AlanCrosby       0.78      0.78      0.78         9\n",
            "   AlexanderSmith       1.00      0.78      0.88         9\n",
            "  BenjaminKangLim       0.78      0.64      0.70        11\n",
            "    BernardHickey       1.00      0.89      0.94         9\n",
            "      BradDorfman       0.91      1.00      0.95        10\n",
            " DarrenSchuettler       1.00      1.00      1.00        13\n",
            "      DavidLawder       0.82      0.69      0.75        13\n",
            "    EdnaFernandes       1.00      0.82      0.90        17\n",
            "      EricAuchard       0.62      0.89      0.73         9\n",
            "   FumikoFujisaki       1.00      0.86      0.92         7\n",
            "   GrahamEarnshaw       0.73      0.89      0.80         9\n",
            " HeatherScoffield       0.92      1.00      0.96        12\n",
            "       JanLopatka       0.78      0.64      0.70        11\n",
            "    JaneMacartney       0.50      0.38      0.43         8\n",
            "     JimGilchrist       1.00      0.92      0.96        12\n",
            "   JoWinterbottom       0.75      0.90      0.82        10\n",
            "         JoeOrtiz       0.78      0.88      0.82         8\n",
            "     JohnMastrini       0.67      0.89      0.76         9\n",
            "     JonathanBirt       0.93      0.93      0.93        14\n",
            "      KarlPenhaul       1.00      1.00      1.00        10\n",
            "        KeithWeir       1.00      0.67      0.80         6\n",
            "   KevinDrawbaugh       0.56      1.00      0.71         5\n",
            "    KevinMorrison       0.85      1.00      0.92        11\n",
            "    KirstinRidley       0.92      0.79      0.85        14\n",
            "KouroshKarimkhany       0.89      0.80      0.84        10\n",
            "        LydiaZajc       1.00      0.78      0.88         9\n",
            "   LynneO'Donnell       0.93      0.93      0.93        14\n",
            "  LynnleyBrowning       1.00      1.00      1.00        10\n",
            "  MarcelMichelson       0.83      1.00      0.91        10\n",
            "     MarkBendeich       0.90      1.00      0.95         9\n",
            "       MartinWolk       0.70      0.70      0.70        10\n",
            "     MatthewBunce       1.00      1.00      1.00        10\n",
            "    MichaelConnor       0.82      0.90      0.86        10\n",
            "       MureDickie       0.78      0.54      0.64        13\n",
            "        NickLouth       0.90      0.90      0.90        10\n",
            "  PatriciaCommins       1.00      1.00      1.00         9\n",
            "    PeterHumphrey       0.45      0.50      0.48        10\n",
            "       PierreTran       1.00      0.85      0.92        13\n",
            "       RobinSidel       0.90      1.00      0.95         9\n",
            "     RogerFillion       1.00      0.88      0.93         8\n",
            "      SamuelPerry       0.88      0.64      0.74        11\n",
            "     SarahDavison       0.56      0.42      0.48        12\n",
            "      ScottHillis       0.43      1.00      0.60         3\n",
            "      SimonCowell       1.00      0.70      0.82        10\n",
            "         TanEeLyn       0.67      0.75      0.71         8\n",
            "   TheresePoletti       1.00      1.00      1.00        10\n",
            "       TimFarrand       0.64      1.00      0.78         9\n",
            "       ToddNissen       0.62      0.56      0.59         9\n",
            "     WilliamKazer       0.45      0.62      0.53         8\n",
            "\n",
            "        micro avg       0.83      0.83      0.83       500\n",
            "        macro avg       0.83      0.83      0.82       500\n",
            "     weighted avg       0.84      0.83      0.83       500\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX6wlU59zVaw"
      },
      "source": [
        "Save results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJQSupEhzVax",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "92ef1d98-f274-4766-a775-e6a9c0594572"
      },
      "source": [
        "joblib.dump(clf_svm, 'svm_results.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['svm_results.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWGWAhmSzVa6"
      },
      "source": [
        "Predict out of sample:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b17BsOqOzVa7"
      },
      "source": [
        "example_y, example_X = y_train[33], X_train[33]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn2k9wDHzVa-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "26437c3b-869d-4787-c177-666bde1c8b03"
      },
      "source": [
        "print('Actual author:', example_y)\n",
        "print('Predicted author:', clf_svm.predict([example_X])[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual author: EricAuchard\n",
            "Predicted author: EricAuchard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "vpRZqHSNzVbB"
      },
      "source": [
        "## <span>Model Selection and Evaluation</span><a id='trad_ml_eval'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiQ52YFLzVbC"
      },
      "source": [
        "Both the `TfidfVectorizer` and `SVC()` estimator take a lot of hyperparameters.  \n",
        "\n",
        "It can be difficult to figure out what the best parameters are.\n",
        "\n",
        "We can use `GridSearchCV` to help figure this out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4wtBtFEzVbC"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wJgG3nczVbH"
      },
      "source": [
        "First we define the options that should be tried out:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBSoERHDzVbJ"
      },
      "source": [
        "clf_search = Pipeline([\n",
        "    ('vect', TfidfVectorizer()),\n",
        "    ('clf', SVC())\n",
        "])\n",
        "parameters = { 'vect__stop_words': ['english'],\n",
        "                'vect__strip_accents': ['unicode'],\n",
        "              'vect__max_features' : [1500],\n",
        "              'vect__ngram_range': [(1,1), (2,2) ],\n",
        "             'clf__gamma' : [0.2, 0.3, 0.4], \n",
        "             'clf__C' : [8, 10, 12],\n",
        "              'clf__kernel' : ['rbf']\n",
        "             }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tp-NLigBzVbL"
      },
      "source": [
        "Run everything:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1VQ-VGjzVbM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "9a509d22-165c-42c1-8d05-ecff30cd59b5"
      },
      "source": [
        "grid = GridSearchCV(clf_search, param_grid=parameters, scoring=make_scorer(f1_score, average='micro'), n_jobs=1)\n",
        "grid.fit(X_train, y_train)    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt2CvCxVzVbQ"
      },
      "source": [
        "*Note:* if you are on a powerful unix system you can set n_jobs to the number of available threads to speed up the calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZg7EZTBzVbQ"
      },
      "source": [
        "print(\"The best parameters are %s with a score of %0.2f\" % (grid.best_params_, grid.best_score_))\n",
        "y_true, y_pred = y_test, grid.predict(X_test)\n",
        "print(metrics.classification_report(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "NvIi77ZczVbW"
      },
      "source": [
        "## <span>Unsupervised</span><a id='trad_ml_unsupervised'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "yGyPcY1JzVbh"
      },
      "source": [
        "### <span>Latent Dirichilet Allocation (LDA)</span><a id='trad_ml_unsupervised_lda'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeVuhs6ZzVbh"
      },
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEfe9WZ-zVbp"
      },
      "source": [
        "Vectorizer (using countvectorizer for the sake of example)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx4JnRCXzVbp"
      },
      "source": [
        "vectorizer = CountVectorizer(strip_accents='unicode',\n",
        "                             lowercase = True,\n",
        "                            max_features = 1500,\n",
        "                            stop_words='english', max_df=0.8)\n",
        "tf_large = vectorizer.fit_transform(clean_paragraphs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC05UAkVzVbs"
      },
      "source": [
        "Run the LDA model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crHxv3OKzVbs"
      },
      "source": [
        "n_topics = 10\n",
        "n_top_words = 25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ECCbQnczVbx"
      },
      "source": [
        "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=10,\n",
        "                                learning_method='online',\n",
        "                                n_jobs=1)\n",
        "lda_fitted = lda.fit_transform(tf_large)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WWs2PfDzVb0"
      },
      "source": [
        "Visualize top words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHgccEPWzVb2"
      },
      "source": [
        "def save_top_words(model, feature_names, n_top_words):\n",
        "    out_list = []\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        out_list.append((topic_idx+1, \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])))\n",
        "    out_df = pd.DataFrame(out_list, columns=['topic_id', 'top_words'])\n",
        "    return out_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUApo9RHzVb_"
      },
      "source": [
        "result_df = save_top_words(lda, vectorizer.get_feature_names(), n_top_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXjV_kHYzVcC"
      },
      "source": [
        "result_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "rtrOd5e2zVci"
      },
      "source": [
        "### <span>pyLDAvis</span><a id='trad_ml_unsupervised_pyLDAvis'></a> [(to top)](#toc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tS_91lazzVck"
      },
      "source": [
        "%matplotlib inline\n",
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn\n",
        "pyLDAvis.enable_notebook()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rxaaZ9fzVco"
      },
      "source": [
        "pyLDAvis.sklearn.prepare(lda, tf_large, vectorizer, n_jobs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqWw0uej5wo-"
      },
      "source": [
        "\n",
        "Credit: [Ties de Kok](https://github.com/TiesdeKok)\n",
        "\n",
        "Repository: [Python NLP](https://github.com/TiesdeKok/Python_NLP_Tutorial)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyoPQUDx5ylY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}