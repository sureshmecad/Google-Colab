{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_Random-Forest.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP10grBeEqkHI6d9WWyNAxv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sureshmecad/Google-Colab/blob/master/1_Random_Forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aovOsjMhukCa"
      },
      "source": [
        "**n_features_int**\n",
        "\n",
        "- The **number of features** when fit is performed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Q6_RhWkvaIS"
      },
      "source": [
        "# import RFE and LogisticRegression\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-tib3pzvStq"
      },
      "source": [
        "# Initialise model variable with LogisticRegression function with solver = 'liblinear'\n",
        "model= LogisticRegression(solver='liblinear')\n",
        "\n",
        "# rfe variable has RFE instance with should have model and n_features_to_select=4 as parameters\n",
        "\n",
        "rfe= RFE(model, n_features_to_select=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xpw_Lh7uLSW"
      },
      "source": [
        "# fit rfe with X and Y\n",
        "\n",
        "fit= rfe.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq35WCJQvCv5"
      },
      "source": [
        "print('Number of selected features', fit.n_features_)\n",
        "print('Selected Features', fit.support_)\n",
        "print('Feature rankings', fit.ranking_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KX_6ya7vHrR"
      },
      "source": [
        "Number of selected features 4\n",
        "\n",
        "Selected Features [ True  True False False False  True  True False]\n",
        "\n",
        "Feature rankings [1 1 2 4 5 1 1 3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr1ecHhJvcua"
      },
      "source": [
        "-----------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7QvURyuxy8f"
      },
      "source": [
        "**get_support**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUs-WiiGxr5n"
      },
      "source": [
        "- To see which **features are important** we can use **get_support** method on the fitted model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QdOutn71Zam"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassfier\n",
        "from sklearn.feature_selection import SelectFromModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbWogSvb1fd2"
      },
      "source": [
        "- In all **feature selection** procedures, it is a **good practice** to select the features by examining only the **training set**. This is to **avoid overfitting.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPXDLLyd1r6_"
      },
      "source": [
        "X_train,y_train,X_test,y_test = train_test_split(data,test_size=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyJ2_xn912U4"
      },
      "source": [
        "- Here I will do the model fitting and feature selection altogether in one line of code.\n",
        "\n",
        " - Firstly, I specify the **random forest instance**, indicating the **number of trees.**\n",
        "\n",
        " - Then I use **selectFromModel** object from sklearn to **automatically select the features.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bt7jqZWAyAxA"
      },
      "source": [
        "sel = SelectFromModel(RandomForestClassifier(n_estimators = 100))\n",
        "sel.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwXo9Pkp2VlJ"
      },
      "source": [
        "- **SelectFromModel** will select those features which **importance is greater** than the **mean importance** of all the features by default, but we can alter this threshold if we want."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy-bd1Sl2jLM"
      },
      "source": [
        "- To see which features are important we can use get_support method on the fitted model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc-kQeou2mzt"
      },
      "source": [
        "sel.get_support()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dfZI5T3yHSI"
      },
      "source": [
        "- It will return an **array of boolean** values.**True** for the features whose importance is greater than the mean importance and **False** for the rest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_72UZwWV0Koy"
      },
      "source": [
        "# We can now make a list and count the selected features.\n",
        "\n",
        "selected_feat= X_train.columns[(sel.get_support())]\n",
        "len(selected_feat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Taz4Azrr0apC"
      },
      "source": [
        "- It will return an Integer representing the number of features selected by the random forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFOsSSvS0QcQ"
      },
      "source": [
        "# To get the name of the features selected\n",
        "\n",
        "print(selected_feat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jht7bVMe1AIt"
      },
      "source": [
        "- It will return the name of the selected features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UW4WPvXd1ENx"
      },
      "source": [
        "# We can also check and plot the distribution of importance\n",
        "\n",
        "pd.series(sel.estimator_,feature_importances_,.ravel()).hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Rvg2GQ01DoK"
      },
      "source": [
        "- It will return a histogram showing the distribution of the features selected using this feature selection technique."
      ]
    }
  ]
}