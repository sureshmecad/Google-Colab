{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_Outlier_Treatment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMGGEUM7y4Yo7TW2c4AIAgN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sureshmecad/Google-Colab/blob/master/1_Outlier_Treatment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XTBdINko9T1"
      },
      "source": [
        "- https://www.pluralsight.com/guides/cleaning-up-data-from-outliers\n",
        "\n",
        "- https://www.analyticsvidhya.com/blog/2021/05/detecting-and-treating-outliers-treating-the-odd-one-out/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjXvST53lQs6"
      },
      "source": [
        "- Once we have identified the outliers, we need to treat them. There are several techniques for this, and we will discuss the most widely used ones below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IUh3h0-lbk4"
      },
      "source": [
        "#### **1) Log Transformation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0WUjDNXlnki"
      },
      "source": [
        "Transformation of the skewed variables may also help correct the distribution of the variables. These could be logarithmic, square root, or square transformations. The most common is the logarithmic transformation, which is done on the 'Loan_amount' variable in the first line of code below. The second and third lines of code print the skewness value before and after the transformation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTTjfVaUlhZh"
      },
      "source": [
        "df[\"Log_Loanamt\"] = df[\"Loan_amount\"].map(lambda i: np.log(i) if i > 0 else 0) \n",
        "print(df['Loan_amount'].skew())\n",
        "print(df['Log_Loanamt'].skew())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vO1RK-Sk7Iw"
      },
      "source": [
        "# Output:\n",
        "2.8146019248106815\n",
        "-0.17792641310111373"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIMXRHIxlwIT"
      },
      "source": [
        "- The above output shows that the skewness value came down from 2.8 to -0.18, confirming that the distribution has been treated for extreme values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_k1wcNBlxRk"
      },
      "source": [
        "#### **2) Replacing Outliers with Median Values**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6E3Y8MSl-1d"
      },
      "source": [
        "In this technique, we replace the extreme values with median values. It is advised to not use mean values as they are affected by outliers. The first line of code below prints the 50th percentile value, or the median, which comes out to be 140. The second line prints the 95th percentile value, which comes out to be around 326. The third line of code below replaces all those values in the 'Loan_amount' variable, which are greater than the 95th percentile, with the median value. Finally, the fourth line prints summary statistics after all these techniques have been employed for outlier treatment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eWg6qjCmFXN"
      },
      "source": [
        "print(df['Loan_amount'].quantile(0.50)) \n",
        "print(df['Loan_amount'].quantile(0.95)) \n",
        "df['Loan_amount'] = np.where(df['Loan_amount'] > 325, 140, df['Loan_amount'])\n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMYQeVvJmJ3n"
      },
      "source": [
        "Output:\n",
        "\n",
        "    140.0\n",
        "    325.7500000000001\n",
        "\n",
        "\n",
        "|       \t| Income       \t| Loan_amount \t| Term_months \t| Credit_score \t| approval_status \t| Age        \t| Log_Loanamt \t|\n",
        "|-------\t|--------------\t|-------------\t|-------------\t|--------------\t|-----------------\t|------------\t|-------------\t|\n",
        "| count \t| 594.000000   \t| 594.000000  \t| 594.000000  \t| 594.000000   \t| 594.000000      \t| 594.000000 \t| 594.000000  \t|\n",
        "| mean  \t| 6112.375421  \t| 144.289562  \t| 366.929293  \t| 0.787879     \t| 0.688552        \t| 50.606061  \t| 4.957050    \t|\n",
        "| std   \t| 3044.257269  \t| 53.033735   \t| 63.705994   \t| 0.409155     \t| 0.463476        \t| 16.266324  \t| 0.494153    \t|\n",
        "| min   \t| 2960.000000  \t| 10.000000   \t| 36.000000   \t| 0.000000     \t| 0.000000        \t| 22.000000  \t| 2.302585    \t|\n",
        "| 25%   \t| 3831.500000  \t| 111.000000  \t| 384.000000  \t| 1.000000     \t| 0.000000        \t| 36.000000  \t| 4.709530    \t|\n",
        "| 50%   \t| 5050.000000  \t| 140.000000  \t| 384.000000  \t| 1.000000     \t| 1.000000        \t| 50.500000  \t| 4.941642    \t|\n",
        "| 75%   \t| 7629.000000  \t| 171.000000  \t| 384.000000  \t| 1.000000     \t| 1.000000        \t| 64.000000  \t| 5.192957    \t|\n",
        "| max   \t| 12681.000000 \t| 324.000000  \t| 504.000000  \t| 1.000000     \t| 1.000000        \t| 80.000000  \t| 6.656727    \t|"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVGqcorWmxW6"
      },
      "source": [
        "#### **3) IQR Score**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbMO2U7Km13z"
      },
      "source": [
        "This technique uses the IQR scores calculated earlier to remove outliers. The rule of thumb is that anything not in the range of (Q1 - 1.5 IQR) and (Q3 + 1.5 IQR) is an outlier, and can be removed. The first line of code below removes outliers based on the IQR range and stores the result in the data frame 'df_out'. The second line prints the shape of this data, which comes out to be 375 observations of 6 variables. This shows that for our data, a lot of records get deleted if we use the IQR method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9Thnk_Im2jy"
      },
      "source": [
        "df_out = df[~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "print(df_out.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY9xOqasm5JD"
      },
      "source": [
        "Output:\n",
        "\n",
        " (375, 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET3CEB_fm_ey"
      },
      "source": [
        "#### **4) Trimming**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aGafUVhnDLs"
      },
      "source": [
        "In this method, we completely remove data points that are outliers. Consider the 'Age' variable, which had a minimum value of 0 and a maximum value of 200. The first line of code below creates an index for all the data points where the age takes these two values. The second line drops these index rows from the data, while the third line of code prints summary statistics for the variable.\n",
        "\n",
        "After trimming, the number of observations is reduced from 600 to 594, and the minimum and maximum values are much more acceptable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5UDVWQ6nGIE"
      },
      "source": [
        "index = df[(df['Age'] >= 100)|(df['Age'] <= 18)].index\n",
        "df.drop(index, inplace=True)\n",
        "df['Age'].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYjN_iHbnG0d"
      },
      "source": [
        "Output:\n",
        "\n",
        "    count    594.000000\n",
        "    mean      50.606061\n",
        "    std       16.266324\n",
        "    min       22.000000\n",
        "    25%       36.000000\n",
        "    50%       50.500000\n",
        "    75%       64.000000\n",
        "    max       80.000000\n",
        "    Name: Age, dtype: float64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb8wk2bJnLwU"
      },
      "source": [
        "#### **5) Quantile-based Flooring and Capping**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-vVDAQRnQq8"
      },
      "source": [
        "In this technique, we will do,\n",
        " - **flooring (e.g., the 10th percentile)** for the **lower values**\n",
        "\n",
        " - **capping (e.g., the 90th percentile)** for the **higher values**.\n",
        " \n",
        " - The lines of code below print the **10th and 90th percentiles** of the variable **'Income'**, respectively. These values will be used for quantile-based flooring and capping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNyudWs1nRaE"
      },
      "source": [
        "print(df['Income'].quantile(0.10))\n",
        "print(df['Income'].quantile(0.90))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkWjAWIinVvt"
      },
      "source": [
        "Output:\n",
        "\n",
        " 2960.1\n",
        " 12681.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRT4iBhunYHd"
      },
      "source": [
        "Now we will remove the outliers, as shown in the lines of code below. Finally, we calculate the skewness value again, which comes out much better now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfWueE2cnVsF"
      },
      "source": [
        "df[\"Income\"] = np.where(df[\"Income\"] < 2960.0, 2960.0, df['Income'])\n",
        "df[\"Income\"] = np.where(df[\"Income\"] > 12681.0, 12681.0, df['Income'])\n",
        "print(df['Income'].skew())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6EZR9M5nawV"
      },
      "source": [
        "Output:\n",
        "\n",
        "  1.04"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}