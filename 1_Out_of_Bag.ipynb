{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_Out of Bag.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPQCVmCY6CPZxPGO88dgiws",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sureshmecad/Google-Colab/blob/master/1_Out_of_Bag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHsxPxd2JtDN"
      },
      "source": [
        "### **Bagging**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55AFfzm-Jwhb"
      },
      "source": [
        "- Suppose we decide to have **S number of trees** in our forest then we first create S datasets of **\"same size as original\"** created from **random resampling of data in T with-replacement** (n times for each dataset).\n",
        "\n",
        "- This will result in {T1, T2, ... TS} datasets. Each of these is called a **bootstrap** dataset. **Due to \"with-replacement\"** every dataset Ti can have **duplicate** data records and Ti can be **missing several data** records from **original datasets**. This is called Bagging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLpV3Co7LdWZ"
      },
      "source": [
        "### **The out-of-bag (oob) error estimate:**\n",
        "\n",
        "\n",
        "- In random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the run, as follows:\n",
        "\n",
        "- Each tree is constructed using a different bootstrap sample from the original data. About **one-third** of the cases are **left out** of the bootstrap sample and **not used** in the construction of the kth tree.\n",
        "\n",
        "- Put each case left out in the construction of the kth tree down the kth tree to get a classification. In this way, a test set classification is obtained for each case in about one-third of the trees. At the end of the run, take j to be the class that got most of the votes every time case n was oob. The proportion of times that j is not equal to the true class of n averaged over all cases is the oob error estimate. This has proven to be unbiased in many tests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvPY24wrWAZw"
      },
      "source": [
        "- Each bagged tree makes use of around 63% of the observations.\n",
        "\n",
        "- An out-of-bag observation is not used by about 37% of the trees."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWynGFKJr4S5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFdXXQRMr8Z6"
      },
      "source": [
        "- https://www.youtube.com/watch?v=t7PbZ1fvy6M ---> How do random forests work?\n",
        "\n",
        "- https://www.youtube.com/watch?v=tdDhyFoSG94 ---> Out of Bag Evaluation in Random Forest\n",
        "\n",
        "- https://www.analyticsvidhya.com/blog/2020/12/out-of-bag-oob-score-in-the-random-forest-algorithm/ ---> Analytics Vidhya"
      ]
    }
  ]
}